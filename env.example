# AI Provider Configuration
# Options: openai, anthropic, ollama, mlx
AI_PROVIDER=ollama

# OpenAI (required if AI_PROVIDER=openai)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic (required if AI_PROVIDER=anthropic)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Ollama (default, no API key required)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:8b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# MLX (Apple Silicon only â€” install deps with: pip install -r requirements-mlx.txt)
# Set AI_PROVIDER=mlx to use. Embeddings still use local sentence-transformers.
MLX_MODEL=mlx-community/Qwen2.5-7B-Instruct-4bit

# Embeddings
# Local embeddings are faster (no API calls) - recommended for better performance
# Set to false only if you need higher quality embeddings via API
USE_LOCAL_EMBEDDINGS=true
LOCAL_EMBEDDING_MODEL=all-mpnet-base-v2
LOCAL_EMBEDDING_DIM=768

# Reranking
CROSS_ENCODER_MODEL=jinaai/jina-reranker-v2-base-multilingual

# CORS Configuration
# Comma-separated list of allowed origins (restrict in production)
CORS_ORIGINS=["http://localhost:8000", "http://127.0.0.1:8000"]

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
